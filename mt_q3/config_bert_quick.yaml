# Quick BERT check (small subset, few epochs) to sample BLEU/ROUGE and timing

seeds:
  random_seed: 42
  numpy_seed: 42
  torch_seed: 42
  torch_cuda_seed: 42

dataset:
  name: "multi30k"
  source_lang: "en"
  target_lang: "de"
  min_freq: 2
  max_length: 50
  sos_token: "<sos>"
  eos_token: "<eos>"
  pad_token: "<pad>"
  unk_token: "<unk>"

embeddings:
  type: "transformer"
  static:
    dim: 256
    trainable: true
  transformer:
    model_name: "distilbert-base-uncased"
    max_length: 64
    freeze: false

seq2seq:
  embedding_dim: 256
  encoder_hidden_dim: 512
  decoder_hidden_dim: 512
  encoder_layers: 2
  decoder_layers: 2
  encoder_dropout: 0.3
  decoder_dropout: 0.3
  attention_dim: 512
  teacher_forcing: 0.5

transformer:
  d_model: 128
  nhead: 2
  num_encoder_layers: 2
  num_decoder_layers: 2
  dim_feedforward: 512
  dropout: 0.1
  activation: "relu"

ablation:
  layers: [2, 4, 6]
  heads: [2, 4, 8]

training:
  batch_size: 128
  learning_rate: 0.001
  epochs: 2          # short run
  gradient_clip: 1.0
  scheduler_factor: 0.5
  scheduler_patience: 3
  scheduler_min_lr: 0.00001
  early_stopping: true
  early_stopping_patience: 5
  optimizer: "adam"
  weight_decay: 0.0001
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  limit_train: 2000   # subset for speed
  limit_val: 500

evaluation:
  beam_search: true
  beam_size: 5
  max_decode_length: 50
  metrics: ["bleu", "rouge"]
  bleu_max_n: 4
  rouge_types: ["rouge1", "rouge2", "rougeL"]

hardware:
  device: "auto"
  num_workers: 0
  pin_memory: true

logging:
  log_interval: 100
  save_dir: "experiments/bert_quick"
  save_last: false
  save_best: true
  tensorboard: false
