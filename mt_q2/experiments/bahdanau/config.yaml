dataset:
  eos_token: <eos>
  max_length: 50
  min_freq: 2
  name: multi30k
  pad_token: <pad>
  sos_token: <sos>
  source_lang: en
  target_lang: de
  unk_token: <unk>
evaluation:
  beam_search: true
  beam_size: 5
  bleu_max_n: 4
  max_decode_length: 50
  metrics:
  - bleu
  - rouge
  - perplexity
  rouge_types:
  - rouge1
  - rouge2
  - rougeL
hardware:
  device: auto
  num_workers: 0
  pin_memory: true
logging:
  log_attention_stats: true
  log_interval: 100
  log_train_loss: true
  log_val_loss: true
  save_dir: experiments
  tensorboard: false
model:
  attention_dim: 512
  attention_types:
  - bahdanau
  - luong
  - scaled_dot
  decoder_dropout: 0.3
  decoder_hidden_dim: 512
  decoder_layers: 2
  embedding_dim: 256
  encoder_bidirectional: true
  encoder_dropout: 0.3
  encoder_hidden_dim: 512
  encoder_layers: 2
seeds:
  numpy_seed: 42
  random_seed: 42
  torch_cuda_seed: 42
  torch_seed: 42
training:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  batch_size: 128
  early_stopping: true
  early_stopping_patience: 5
  epochs: 20
  gradient_clip: 1.0
  learning_rate: 0.001
  optimizer: adam
  save_best_only: true
  scheduler: reduce_on_plateau
  scheduler_factor: 0.5
  scheduler_min_lr: 1.0e-05
  scheduler_patience: 3
  teacher_forcing_ratio: 0.5
  validate_every: 1
  weight_decay: 0.0001
visualization:
  analyze_entropy: true
  analyze_sharpness: true
  attention_heatmap:
    cmap: viridis
    dpi: 300
    figsize:
    - 12
    - 8
    save_format: png
  num_samples: 5
